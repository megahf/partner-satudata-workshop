---
AWSTemplateFormatVersion: '2010-09-09'
Description: AWS CloudFormation template for Satu Data - Executive Dashboard workshop.

Metadata:
  License:
    Description: 'Copyright 2023 Amazon.com, Inc. and its affiliates. All Rights Reserved.SPDX-License-Identifier: MIT-0'

Resources:

################## CLOUD9 #################
  Cloud9IAMRole:
    Type: AWS::IAM::Role
    Properties:
      Tags:
        - Key: Environment
          Value: AWS Example
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - ec2.amazonaws.com
            - ssm.amazonaws.com
            - s3.amazonaws.com
          Action:
          - sts:AssumeRole
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/AmazonS3FullAccess
      Path: "/"

  Cloud9InstanceProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Path: "/"
      Roles:
      - Ref: Cloud9IAMRole

  Cloud9Instance:
    Description: "-"
    Type: AWS::Cloud9::EnvironmentEC2
    Properties:
      Description: AWS Cloud9 instance for satu data workshop
      AutomaticStopTimeMinutes: 3600
      InstanceType: t3.small
      Name: Cloud9 Instance
      Tags: 
        - 
          Key: SSMBootstrap
          Value: Active
        - 
          Key: Environment
          Value: AWS Example



  ################## S3  #################

  SatuDataS3Bucket:
      Type: AWS::S3::Bucket
      Properties:
        BucketName: !Sub "satudata-workshop-2023-${AWS::AccountId}"


  AthenaQueryS3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Sub "satudata-athena-output-${AWS::AccountId}"

  ################## GLUE  #################

  GlueIAMRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: glue-job-s3-permission-role1
      AssumeRolePolicyDocument: 
        Version: "2012-10-17"
        Statement: 
          - 
            Effect: "Allow"
            Principal: 
              Service: 
                - "glue.amazonaws.com"
            Action: 
              - "sts:AssumeRole"
      Policies:
        - PolicyName: s3-satudata-policy
          PolicyDocument:
             Version: 2012-10-17
             Statement:
                - Sid: ListBucket
                  Effect: Allow
                  Action:
                    - "s3:ListAllMyBuckets"
                  Resource: '*'
                - Sid: SatuDataBuckets
                  Effect: Allow
                  Action: "s3:*"
                  Resource:
                    - !Sub "arn:aws:s3:::satudata-workshop-2023-${AWS::AccountId}"
                    - !Sub "arn:aws:s3:::satudata-workshop-2023-${AWS::AccountId}/*"
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
      Path: "/"

  GlueRawDatabase:
    Type: AWS::Glue::Database
    Properties: 
      CatalogId: !Ref AWS::AccountId
      DatabaseInput: 
        Name: satudata-raw-db

  GlueRawCrawler:
    Type: AWS::Glue::Crawler
    Properties: 
      DatabaseName: !Ref GlueRawDatabase
      Name: satudata-raw-crawler
      Role: !Ref GlueIAMRole
      TablePrefix: raw-
      Targets: 
        S3Targets:
          - Path: !Sub "${SatuDataS3Bucket}/raw/"


  ################## LAMBDA LAB  #################

  LambdaIAMRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: satudata-excel-lambda-role
      AssumeRolePolicyDocument: 
        Version: "2012-10-17"
        Statement: 
          - 
            Effect: "Allow"
            Principal: 
              Service: 
                - "lambda.amazonaws.com"
            Action: 
              - "sts:AssumeRole"
      Policies:
        - PolicyName: s3-satudata-policy
          PolicyDocument:
             Version: 2012-10-17
             Statement:
                - Sid: ListBucket
                  Effect: Allow
                  Action:
                    - "s3:ListAllMyBuckets"
                  Resource: '*'
                - Sid: SatuDataBuckets
                  Effect: Allow
                  Action: "s3:*"
                  Resource:
                    - !Sub "arn:aws:s3:::satudata-workshop-2023-${AWS::AccountId}"
                    - !Sub "arn:aws:s3:::satudata-workshop-2023-${AWS::AccountId}/*"
                    - !Sub "arn:aws:s3:::satudata-excelfile-${AWS::AccountId}"
                    - !Sub "arn:aws:s3:::satudata-excelfile-${AWS::AccountId}/*"
      ManagedPolicyArns:
      - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Path: "/"


  ExcelToCSVLambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      FunctionName: excel-to-csv-function
      Runtime: python3.9
      Role: !GetAtt LambdaIAMRole.Arn
      Timeout: 300
      Description: Invoke when new excel file is uploaded to source S3 bucket
      Environment:
        Variables:
          DATA_TARGET_BUCKET: !Ref SatuDataS3Bucket
      Handler: index.lambda_handler
      Code:
        ZipFile: |
          import os
          import boto3
          import pandas as pd
          import json


          s3 = boto3.resource('s3')
          s3_dest = boto3.resource('s3')
          s3_client = boto3.client('s3')


          def lambda_handler(event, context):
              data_target_bucket = os.environ['DATA_TARGET_BUCKET']            
              data_source_bucket = event['Records'][0]['s3']['bucket']['name'] 
              data_source_key = event['Records'][0]['s3']['object']['key']   
              print(event['Records'][0]['s3'])

              os.chdir('/tmp')
              source_bucket_obj = s3.Bucket(data_source_bucket)

              # check if root
              if "/" in data_source_key: 
                  print("in root")
                  # get file name
                  print("filename=" + data_source_key.rsplit('/', 1)[1])
                  file_name=data_source_key.rsplit('/', 1)[1]
                  # get path s3
                  print("S3=" + data_source_key.rsplit('/', 1)[0])
                  file_path=data_source_key.rsplit('/', 1)[0]+"/"
                  print("File path=" + file_path)
                  objects = source_bucket_obj.objects.filter(Prefix=file_path)
                  for obj in objects:
                      path, filename = os.path.split(obj.key)
                      if file_name in data_source_key: 
                          source_bucket_obj.download_file(obj.key, file_name)
                  data_source_key=file_name
              else:
                  source_bucket_obj.download_file(data_source_key, data_source_key)
                  file_path=""
                  
              # check dest folder first
              s3_dest_bucket = s3_dest.Bucket(data_target_bucket)
              c=0
              objects = s3_dest_bucket.objects.filter(Prefix=file_path)
              for obj in objects:
                  c+=1
              
              
              csv_filename = os.path.splitext(os.path.basename(data_source_key))[0] + '.csv'
                  
              df = pd.read_excel(data_source_key)
              opd_name = df.loc[df.iloc[:, 0] == 'PERANGKAT DAERAH'].dropna(axis=1).iloc[:,-1:].iloc[0,0]
              df_new = df.loc[df.iloc[:, 0].str.isnumeric() == True]
              df_clean = df_new.loc[:, df_new.iloc[0].isin(['1','2','3','4','5','6','7','8','9','10','11','12','13','14','15','16',])]
              df_clean = df_clean.iloc[1: , :]
              df_final = df_clean.set_axis(['Nomor Urut', 'Kode Barang', 'Register', 'Kode Sensus', 'Nama Barang','Merk','Nomor Sertifikat','Bahan','Cara Perolehan','Tahun Perolehan','Ukuran Barang','Satuan','Keadaan Barang','Jumlah Barang','Nilai Barang','Keterangan',], axis=1, inplace=False)
              df_final['OPD'] = opd_name
              df_final.to_csv(csv_filename, encoding='utf-8', index=False)
                  
              s3_client.upload_file(csv_filename, data_target_bucket, file_path+csv_filename)
              
              try:
                  # delete after success
                  response = s3_client.delete_object(
                      Bucket=data_source_bucket,
                      Key=event['Records'][0]['s3']['object']['key']
                  )
              except ClientError as e:
                  print("Error Deleting File")
                  
              return {
                'statusCode': 200,
                'body': json.dumps('Converted')
                  }

  ProcessingLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: 'lambda:InvokeFunction'
      FunctionName: !Ref ExcelToCSVLambdaFunction
      Principal: s3.amazonaws.com
      SourceArn: !Sub 'arn:aws:s3:::satudata-excelfile-${AWS::AccountId}'
      SourceAccount: !Ref AWS::AccountId


  SatuDataExcelS3Bucket:
      Type: AWS::S3::Bucket
      DependsOn:
        - ProcessingLambdaPermission
      Properties:
        BucketName: !Sub "satudata-excelfile-${AWS::AccountId}"
        NotificationConfiguration:
          LambdaConfigurations:
            - Event: s3:ObjectCreated:*
              Function: !GetAtt ExcelToCSVLambdaFunction.Arn
              Filter:
                S3Key:
                  Rules:
                  - Name: suffix
                    Value: .xlsx



